{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Sequential, regularizers\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Embedding, LSTM, GRU\n",
    "from tensorflow.keras.optimizers.legacy import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.utils import timeseries_dataset_from_array\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "\n",
    "from keras.regularizers import L1, L2, L1L2\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data & column groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_DATA_PATH = \"data.v3/daily\" \n",
    "\n",
    "df = pd.read_parquet(os.path.join(DAILY_DATA_PATH, \"daily_flights_and_weather_merged.parquet\"))\n",
    "\n",
    "# Flights column groups\n",
    "flights_terminal_cols = ['flights_arr_A', 'flights_arr_B', 'flights_arr_C', 'flights_arr_D', 'flights_arr_E',\n",
    "                         'flights_dep_A', 'flights_dep_B', 'flights_dep_C', 'flights_dep_D', 'flights_dep_E']\n",
    "\n",
    "flights_non_terminal_cols = ['flights_total', 'flights_cancel', 'flights_delay', 'flights_ontime',\n",
    "                             'flights_arr_ontime', 'flights_arr_delay', 'flights_arr_cancel',\n",
    "                             'flights_dep_ontime', 'flights_dep_delay', 'flights_dep_cancel']\n",
    "\n",
    "flights_percentage_cols = ['flights_cancel_pct', 'flights_delay_pct', 'flights_ontime_pct',\n",
    "                            'flights_arr_delay_pct', 'flights_arr_ontime_pct', 'flights_arr_cancel_pct',\n",
    "                            'flights_dep_delay_pct', 'flights_dep_ontime_pct', 'flights_dep_cancel_pct']\n",
    "\n",
    "# Date column groups\n",
    "date_cols = ['date', 'covid', 'ordinal_date', 'year', 'month', 'day_of_month', 'day_of_week', 'season', 'holiday', 'halloween', 'xmas_eve', 'new_years_eve', 'jan_2', 'jan_3', 'day_before_easter', 'days_until_xmas', 'days_until_thanksgiving', 'days_until_july_4th', 'days_until_labor_day', 'days_until_memorial_day']\n",
    "\n",
    "# Weather column groups\n",
    "weather_cols = ['wx_temperature_max', 'wx_temperature_min', 'wx_apcp', 'wx_prate', 'wx_asnow', 'wx_frozr', 'wx_vis', 'wx_gust', 'wx_maxref', 'wx_cape', 'wx_lftx', 'wx_wind_speed', 'wx_wind_direction']\n",
    "\n",
    "# Lag column groups\n",
    "lag_cols =  ['flights_total_lag_1', 'flights_total_lag_2', 'flights_total_lag_3', 'flights_total_lag_4', 'flights_total_lag_5', 'flights_total_lag_6', 'flights_total_lag_7', 'flights_cancel_lag_1', 'flights_cancel_lag_2', 'flights_cancel_lag_3', 'flights_cancel_lag_4', 'flights_cancel_lag_5', 'flights_cancel_lag_6', 'flights_cancel_lag_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split - \"flights_ontime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_full shape: (1516, 47)\n",
      "y_train_full shape: (1516,)\n",
      "X_train shape: (1364, 47)\n",
      "y_train shape: (1364,)\n",
      "X_Test shape: (169, 47)\n",
      "y_Test shape: (169,)\n"
     ]
    }
   ],
   "source": [
    "# Select features and targets\n",
    "train_features = ['random'] + date_cols + weather_cols + lag_cols\n",
    "targets = flights_non_terminal_cols + flights_percentage_cols\n",
    "\n",
    "# Create X and y\n",
    "X = df[train_features].drop('date', axis=1)\n",
    "y = df[targets]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y['flights_ontime'], test_size=0.1, random_state=42)\n",
    "\n",
    "# Split data into X_train_rull and y_train_full into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print shapes\n",
    "print(\"X_train_full shape:\", X_train_full.shape)\n",
    "print(\"y_train_full shape:\", y_train_full.shape)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "print(\"X_Test shape:\", X_test.shape)\n",
    "print(\"y_Test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS FOR DENSE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['random', 'covid', 'ordinal_date', 'year', 'month', 'day_of_month', 'day_of_week', 'season', 'holiday', 'halloween', 'xmas_eve', 'new_years_eve', 'jan_2', 'jan_3', 'day_before_easter', 'days_until_xmas', 'days_until_thanksgiving', 'days_until_july_4th', 'days_until_labor_day', 'days_until_memorial_day', 'wx_temperature_max', 'wx_temperature_min', 'wx_apcp', 'wx_prate', 'wx_asnow', 'wx_frozr', 'wx_vis', 'wx_gust', 'wx_maxref', 'wx_cape', 'wx_lftx', 'wx_wind_speed', 'wx_wind_direction', 'flights_total_lag_1', 'flights_total_lag_2', 'flights_total_lag_3', 'flights_total_lag_4', 'flights_total_lag_5', 'flights_total_lag_6', 'flights_total_lag_7', 'flights_cancel_lag_1', 'flights_cancel_lag_2', 'flights_cancel_lag_3', 'flights_cancel_lag_4', 'flights_cancel_lag_5', 'flights_cancel_lag_6', 'flights_cancel_lag_7']\n",
      "Target columns: ['flights_total', 'flights_cancel', 'flights_delay', 'flights_ontime', 'flights_arr_ontime', 'flights_arr_delay', 'flights_arr_cancel', 'flights_dep_ontime', 'flights_dep_delay', 'flights_dep_cancel', 'flights_cancel_pct', 'flights_delay_pct', 'flights_ontime_pct', 'flights_arr_delay_pct', 'flights_arr_ontime_pct', 'flights_arr_cancel_pct', 'flights_dep_delay_pct', 'flights_dep_ontime_pct', 'flights_dep_cancel_pct']\n",
      "\n",
      "Unique data types in X\n",
      "float64    23\n",
      "object     11\n",
      "int64       7\n",
      "float32     4\n",
      "int32       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns to one-hot-encode: ['covid', 'month', 'day_of_week', 'season', 'holiday', 'halloween', 'xmas_eve', 'new_years_eve', 'jan_2', 'jan_3', 'day_before_easter']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Feature names: {X.columns.tolist()}\")\n",
    "print(f\"Target columns: {y.columns.tolist()}\", end=\"\\n\\n\")\n",
    "print(\"Unique data types in X\", X.dtypes.value_counts(), sep = '\\n')\n",
    "\n",
    "# Identify categorical and numeric columns in X\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include = ['float64', 'float32', 'int32', 'int64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical columns to one-hot-encode: {categorical_cols}\")\n",
    "\n",
    "# Fit transformers to the training data\n",
    "f_scaler = StandardScaler()\n",
    "f_scaler.fit(X_train[numeric_cols])\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Some observed holidays may not be in the training data\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "t_scaler = StandardScaler()\n",
    "t_scaler.fit(y_train.values.reshape(-1, 1)) # reshape y_train to be 2D\n",
    "\n",
    "# Define preprocessor\n",
    "def preprocess(features, target, set_global_scaler = False):\n",
    "    global global_targer_scaler\n",
    "\n",
    "    scaled_features = f_scaler.transform(features[numeric_cols])\n",
    "    encoded_features = ohe.transform(features[categorical_cols])\n",
    "    scaled_target = t_scaler.transform(target.values.reshape(-1, 1))\n",
    "    processed_features = np.concatenate([scaled_features, encoded_features], axis=1)\n",
    "\n",
    "    if set_global_scaler:\n",
    "        global_targer_scaler = t_scaler\n",
    "\n",
    "    return processed_features, scaled_target\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_d, y_train_d = preprocess(X_train, y_train, set_global_scaler=True)\n",
    "X_val_d, y_val_d = preprocess(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT WITH 1 NEURON \"LINEAR MODEL\"\n",
    "\n",
    "The goal of this section is to simulate linear regression using a neural newtork with one neuron and no activation function. We'll use L2 regularization to simulate ridge regression and compare results to those from Sklearn's lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow datasets (not timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow datasets\n",
    "train_ds_flights_ontime_d = Dataset.from_tensor_slices((X_train_d, y_train_d)).shuffle(len(X_train_d))\n",
    "val_ds_flights_ontime_d = Dataset.from_tensor_slices((X_val_d, y_val_d)).shuffle(len(X_val_d))\n",
    "\n",
    "# Batch and prefetch\n",
    "batch_size = 32\n",
    "train_ds_flights_ontime_d = train_ds_flights_ontime_d.batch(batch_size).prefetch(AUTOTUNE)\n",
    "val_ds_flights_ontime_d = val_ds_flights_ontime_d.batch(batch_size).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create R-squared metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    y_true_inv = tf.numpy_function(global_targer_scaler.inverse_transform, [y_true], tf.float32)\n",
    "    y_pred_inv = tf.numpy_function(global_targer_scaler.inverse_transform, [y_pred], tf.float32)\n",
    "    SS_res =  K.sum(K.square(y_true_inv - y_pred_inv)) \n",
    "    SS_tot = K.sum(K.square(y_true_inv - K.mean(y_true_inv))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Neuron Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 14s]\n",
      "val_loss: 0.4528622478246689\n",
      "\n",
      "Best val_loss So Far: 0.4294688552618027\n",
      "Total elapsed time: 00h 02m 33s\n"
     ]
    }
   ],
   "source": [
    "# Callbacks & Tensorboard Setup\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Keras Tuner Design\n",
    "def model_builder(hp):\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    l1_regularization = hp.Float('l1_regularization', min_value=1e-5, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "    l2_regularization =  hp.Float('l2_regularization', min_value=1e-5, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(units = 1, \n",
    "            input_dim=X_train_d.shape[1], \n",
    "            kernel_regularizer=L1L2(l1_regularization, l2_regularization))\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                loss='mean_squared_error', \n",
    "                metrics=['mean_absolute_error', r_squared])\n",
    "    return model\n",
    "\n",
    "# Create a Keras Tuner\n",
    "dense_lr_tuner = kt.RandomSearch(\n",
    "    hypermodel = model_builder,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='logs/flights_ontime/dense_lr/',\n",
    "    project_name='tuner',\n",
    "    overwrite = True\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters\n",
    "dense_lr_tuner.search(train_ds_flights_ontime_d, \n",
    "             validation_data=val_ds_flights_ontime_d, \n",
    "             epochs=500, \n",
    "             callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Neuron model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in logs/flights_ontime/dense_lr/tuner\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0015511343794524048\n",
      "l1_regularization: 1.9021653410411186e-05\n",
      "l2_regularization: 0.0003770290850368308\n",
      "Score: 0.4294688552618027\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0033946715472269115\n",
      "l1_regularization: 0.00044896982083573897\n",
      "l2_regularization: 0.004091965398247103\n",
      "Score: 0.43117551505565643\n",
      "\n",
      "Trial 14 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0037561240060219256\n",
      "l1_regularization: 3.319105045503531e-05\n",
      "l2_regularization: 0.00667842270347455\n",
      "Score: 0.4329136162996292\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.005441667012386122\n",
      "l1_regularization: 3.906857576488224e-05\n",
      "l2_regularization: 0.002584585097692256\n",
      "Score: 0.4344650208950043\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00038311174343190024\n",
      "l1_regularization: 1.2901888244893854e-05\n",
      "l2_regularization: 0.0007334847697174814\n",
      "Score: 0.43713730573654175\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00013995710682620886\n",
      "l1_regularization: 1.0305514323318857e-05\n",
      "l2_regularization: 4.187148572998108e-05\n",
      "Score: 0.4372638911008835\n",
      "\n",
      "Trial 10 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.002219262552270968\n",
      "l1_regularization: 1.0942053618920959e-05\n",
      "l2_regularization: 0.0030907069973227855\n",
      "Score: 0.43738164007663727\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0005095016426245528\n",
      "l1_regularization: 1.1744090880907817e-05\n",
      "l2_regularization: 0.001674246995864395\n",
      "Score: 0.43755970895290375\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00013611464942586763\n",
      "l1_regularization: 0.00011448945821103439\n",
      "l2_regularization: 5.814989485176248e-05\n",
      "Score: 0.43891894817352295\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0008151719073899244\n",
      "l1_regularization: 2.3353611980767127e-05\n",
      "l2_regularization: 0.008775363154103792\n",
      "Score: 0.4395657181739807\n"
     ]
    }
   ],
   "source": [
    "# Print hyperparameters for the 10 best trials\n",
    "dense_lr_tuner.results_summary(num_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Hyperparameters:\n",
      "- Learning Rate: 0.002\n",
      "- L1 Regularization: 0.00002\n",
      "- L2 Regularization: 0.00038\n",
      "\n",
      "Loss and Metrics for Best Trial:\n",
      "- Validation Loss: 0.43\n",
      "- Validation MAE: 0.44\n",
      "- Validation R^2: 0.633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get best hyperparameters\n",
    "best_hps = dense_lr_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "learning_rate = best_hps.get('learning_rate')\n",
    "l1_amount = best_hps.get('l1_regularization')\n",
    "l2_amount = best_hps.get('l2_regularization')\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Hyperparameters:\n",
    "- Learning Rate: {learning_rate:.3f}\n",
    "- L1 Regularization: {l1_amount:.5f}\n",
    "- L2 Regularization: {l2_amount:.5f}\n",
    "\"\"\")\n",
    "\n",
    "# Get best trial\n",
    "best_trial = dense_lr_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "\n",
    "# Best trial metrics\n",
    "val_loss = best_trial.metrics.get_best_value('val_loss')\n",
    "val_mae = best_trial.metrics.get_best_value('val_mean_absolute_error')\n",
    "val_r2 = best_trial.metrics.get_best_value('val_r_squared')\n",
    "\n",
    "print(f\"\"\"Loss and Metrics for Best Trial:\n",
    "- Validation Loss: {val_loss:.2f}\n",
    "- Validation MAE: {val_mae:.2f}\n",
    "- Validation R^2: {val_r2:.3f}\n",
    "\"\"\")\n",
    "\n",
    "# Tensorboard directory setup\n",
    "!rm -rf ./logs/flights_ontime/dense_lr/tensorboard/ \n",
    "log_dir = \"logs/flights_ontime/dense_lr/tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train the model with the optimal hyperparameters\n",
    "model = dense_lr_tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_ds_flights_ontime_d, \n",
    "                    validation_data=val_ds_flights_ontime_d, \n",
    "                    epochs=500, \n",
    "                    callbacks=[early_stopping, tensorboard_callback],\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1-Neuron TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bffad48b42e29019\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bffad48b42e29019\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/flights_ontime/dense_lr/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT WITH A SHALLOW DENSE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 03s]\n",
      "val_loss: 0.4783097803592682\n",
      "\n",
      "Best val_loss So Far: 0.4009020924568176\n",
      "Total elapsed time: 00h 07m 53s\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int('n_hidden', min_value=1, max_value=2, default=2)\n",
    "    n_neurons = hp.Int('n_neurons', min_value=1, max_value=32, default=16)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.02, max_value=0.03, default=0.0)\n",
    "    l2_regularization =  hp.Float('l2_regularization', min_value=1e-5, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=n_neurons, \n",
    "                    activation='relu', \n",
    "                    kernel_regularizer=L2(l2_regularization)))\n",
    "    \n",
    "    for layer in range(n_hidden-1):\n",
    "        model.add(Dense(units=n_neurons, \n",
    "                        activation='relu', \n",
    "                        kernel_regularizer=L2(l2_regularization)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    if n_hidden > 0:\n",
    "        model.add(Dense(units=n_neurons, \n",
    "                        activation='relu', \n",
    "                        kernel_regularizer=L2(l2_regularization)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                  loss='mean_squared_error', \n",
    "                  metrics=['mean_absolute_error'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel = build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    num_initial_points=2,\n",
    "    directory = \"flights_ontime\",\n",
    "    project_name = \"flights_ontime_shallow_dense_fit\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "dense_shallow_tuner = kt.RandomSearch(\n",
    "    hypermodel = build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    directory = \"logs/flights_ontime/dense_shallow/\",\n",
    "    project_name = \"tuner\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "dense_shallow_tuner.search(train_ds_flights_ontime_d, \n",
    "             epochs=500, \n",
    "             validation_data=val_ds_flights_ontime_d, \n",
    "             callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Dense NN model perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in logs/flights_ontime/dense_shallow/tuner\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 010 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 30\n",
      "learning_rate: 0.006715083131370706\n",
      "dropout_rate: 0.023519556922148722\n",
      "l2_regularization: 0.0001572568838800907\n",
      "Score: 0.4009020924568176\n",
      "\n",
      "Trial 017 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 3\n",
      "learning_rate: 0.002605398610948284\n",
      "dropout_rate: 0.02128017052812929\n",
      "l2_regularization: 1.733380806051542e-05\n",
      "Score: 0.40239638090133667\n",
      "\n",
      "Trial 074 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 1\n",
      "n_neurons: 20\n",
      "learning_rate: 0.001468267917477422\n",
      "dropout_rate: 0.02327180712115707\n",
      "l2_regularization: 2.5663603752396305e-05\n",
      "Score: 0.4088706821203232\n",
      "\n",
      "Trial 023 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 1\n",
      "n_neurons: 20\n",
      "learning_rate: 0.0077158177708831835\n",
      "dropout_rate: 0.023931066081892202\n",
      "l2_regularization: 0.0001342648611028202\n",
      "Score: 0.41031041741371155\n",
      "\n",
      "Trial 007 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 21\n",
      "learning_rate: 0.005666365603069349\n",
      "dropout_rate: 0.02378547054621139\n",
      "l2_regularization: 6.181351820451506e-05\n",
      "Score: 0.4142153859138489\n",
      "\n",
      "Trial 009 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 1\n",
      "n_neurons: 5\n",
      "learning_rate: 0.003993492840731953\n",
      "dropout_rate: 0.029049008086799163\n",
      "l2_regularization: 7.014213089908583e-05\n",
      "Score: 0.4144311994314194\n",
      "\n",
      "Trial 080 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 1\n",
      "n_neurons: 16\n",
      "learning_rate: 0.008974135409611785\n",
      "dropout_rate: 0.022057287780026635\n",
      "l2_regularization: 0.000323725141549069\n",
      "Score: 0.4194975346326828\n",
      "\n",
      "Trial 066 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 17\n",
      "learning_rate: 0.0018156534289753442\n",
      "dropout_rate: 0.02628659944238511\n",
      "l2_regularization: 3.6657764947184424e-05\n",
      "Score: 0.4205448478460312\n",
      "\n",
      "Trial 091 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 28\n",
      "learning_rate: 0.0018539278556818578\n",
      "dropout_rate: 0.022039395346397622\n",
      "l2_regularization: 0.00016468388199075124\n",
      "Score: 0.42123058438301086\n",
      "\n",
      "Trial 055 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 1\n",
      "n_neurons: 17\n",
      "learning_rate: 0.0019595524921753255\n",
      "dropout_rate: 0.027262425981464534\n",
      "l2_regularization: 0.00025643106755098797\n",
      "Score: 0.4213596284389496\n"
     ]
    }
   ],
   "source": [
    "# Print hyperparameters for the 10 best trials\n",
    "dense_shallow_tuner.results_summary(num_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Hyperparameters:\n",
      "- Number of Hidden Layers: 2\n",
      "- Number of Neurons: 30\n",
      "- Learning Rate: 0.007\n",
      "- Dropout Rate: 0.024\n",
      "- L2 Regularization: 0.00016\n",
      "\n",
      "Loss and Metrics for Best Trial:\n",
      "- Validation Loss: 0.40\n",
      "- Validation MAE: 0.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_hps = dense_shallow_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "# Get best hyperparameters\n",
    "n_hidden = best_hps.get('n_hidden')\n",
    "n_neurons = best_hps.get('n_neurons')\n",
    "learning_rate = best_hps.get('learning_rate')\n",
    "dropout_rate = best_hps.get('dropout_rate')\n",
    "l2_amount = best_hps.get('l2_regularization')\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Hyperparameters:\n",
    "- Number of Hidden Layers: {n_hidden}\n",
    "- Number of Neurons: {n_neurons}\n",
    "- Learning Rate: {learning_rate:.3f}\n",
    "- Dropout Rate: {dropout_rate:.3f}\n",
    "- L2 Regularization: {l2_amount:.5f}\n",
    "\"\"\")\n",
    "\n",
    "# Get best trial\n",
    "best_trial = dense_shallow_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "\n",
    "# Best trial metrics\n",
    "val_loss = best_trial.metrics.get_best_value('val_loss')\n",
    "val_mae = best_trial.metrics.get_best_value('val_mean_absolute_error')\n",
    "\n",
    "print(f\"\"\"Loss and Metrics for Best Trial:\n",
    "- Validation Loss: {val_loss:.2f}\n",
    "- Validation MAE: {val_mae:.2f}\n",
    "\"\"\")\n",
    "\n",
    "# Tensorboard directory setup\n",
    "!rm -rf ./logs/flights_ontime/dense_shallow/tensorboard/ \n",
    "log_dir = \"logs/flights_ontime/dense_shallow/tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Change early stopping patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Train the model with the optimal hyperparameters\n",
    "model = dense_shallow_tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_ds_flights_ontime_d, \n",
    "                    validation_data=val_ds_flights_ontime_d, \n",
    "                    epochs=500, \n",
    "                    callbacks=[early_stopping, tensorboard_callback],\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT WITH RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove lag variables from X train, val, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_X_train_full = X_train_full.drop(lag_cols, axis=1)\n",
    "rnn_X_train = X_train.drop(lag_cols, axis=1)\n",
    "rnn_X_val = X_val.drop(lag_cols, axis=1)\n",
    "rnn_X_test = X_test.drop(lag_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN column transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_numeric_cols = [col for col in numeric_cols if col not in lag_cols]\n",
    "\n",
    "# Fit transformers to the training data\n",
    "rnn_f_scaler = StandardScaler()\n",
    "rnn_f_scaler.fit(rnn_X_train[rnn_numeric_cols])\n",
    "\n",
    "# Create a function to preprocess TensorFlow datasets\n",
    "def rnn_preprocess(features, target):\n",
    "    scaled_features = rnn_f_scaler.transform(features[rnn_numeric_cols])\n",
    "    encoded_features = ohe.transform(features[categorical_cols])\n",
    "    scaled_target = t_scaler.transform(target.values.reshape(-1, 1))\n",
    "    processed_features = np.concatenate([scaled_features, encoded_features], axis=1)\n",
    "    return processed_features, scaled_target\n",
    "\n",
    "# Transform the data\n",
    "X_train_rnn, y_train_rnn = rnn_preprocess(X_train, y_train)\n",
    "X_val_rnn, y_val_rnn = rnn_preprocess(X_val, y_val)\n",
    "X_test_rnn, y_test_rnn = rnn_preprocess(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create timeseries datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "seq_length = 7\n",
    "batch_size = 32\n",
    "\n",
    "train_rnn = timeseries_dataset_from_array(\n",
    "    data = X_train_rnn, \n",
    "    targets = y_train_rnn,\n",
    "    sequence_length = seq_length,\n",
    "    sequence_stride = 1,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "val_rnn = timeseries_dataset_from_array(\n",
    "    data = X_val_rnn, \n",
    "    targets = y_val_rnn[seq_length-1:],\n",
    "    sequence_length = seq_length,\n",
    "    sequence_stride = 1,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "test_rnn = timeseries_dataset_from_array(\n",
    "    data = X_test_rnn, \n",
    "    targets = y_test_rnn[seq_length-1:],\n",
    "    sequence_length = seq_length,\n",
    "    sequence_stride = 1,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Using a single RNN\n",
    "\n",
    "RETURN HERE: \n",
    "1. COPY SHALLOW DENSE NETWORK BUILD FUNCTION. EXPLORE THE SAME ARCHITECTURE HYPERPARAMETERS, BUT WITH AN RNN. \n",
    "2. ADD TENSOBOARD AND EARLY STOPPING\n",
    "3. ADD L1 AND L2 REGULARIZATION\n",
    "4. LEARN THE TENSORBOARD FEATURES\n",
    "5. WRITE CONCLUSIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.0267 - mean_absolute_error: 1.6237 - val_loss: 2.4874 - val_mean_absolute_error: 1.0796\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.5697 - mean_absolute_error: 0.8779 - val_loss: 1.7633 - val_mean_absolute_error: 0.9113\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.2149 - mean_absolute_error: 0.7975 - val_loss: 1.6112 - val_mean_absolute_error: 0.8859\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.1201 - mean_absolute_error: 0.7732 - val_loss: 1.5329 - val_mean_absolute_error: 0.8690\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0803 - mean_absolute_error: 0.7640 - val_loss: 1.4938 - val_mean_absolute_error: 0.8598\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0582 - mean_absolute_error: 0.7601 - val_loss: 1.4717 - val_mean_absolute_error: 0.8557\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0462 - mean_absolute_error: 0.7584 - val_loss: 1.4587 - val_mean_absolute_error: 0.8538\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0398 - mean_absolute_error: 0.7579 - val_loss: 1.4530 - val_mean_absolute_error: 0.8541\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0353 - mean_absolute_error: 0.7576 - val_loss: 1.4466 - val_mean_absolute_error: 0.8540\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0321 - mean_absolute_error: 0.7575 - val_loss: 1.4427 - val_mean_absolute_error: 0.8542\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0298 - mean_absolute_error: 0.7574 - val_loss: 1.4384 - val_mean_absolute_error: 0.8541\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0280 - mean_absolute_error: 0.7573 - val_loss: 1.4345 - val_mean_absolute_error: 0.8544\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0264 - mean_absolute_error: 0.7571 - val_loss: 1.4321 - val_mean_absolute_error: 0.8548\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0251 - mean_absolute_error: 0.7570 - val_loss: 1.4324 - val_mean_absolute_error: 0.8553\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0241 - mean_absolute_error: 0.7570 - val_loss: 1.4327 - val_mean_absolute_error: 0.8557\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0233 - mean_absolute_error: 0.7570 - val_loss: 1.4281 - val_mean_absolute_error: 0.8558\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0223 - mean_absolute_error: 0.7569 - val_loss: 1.4316 - val_mean_absolute_error: 0.8563\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0219 - mean_absolute_error: 0.7570 - val_loss: 1.4327 - val_mean_absolute_error: 0.8565\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0214 - mean_absolute_error: 0.7569 - val_loss: 1.4322 - val_mean_absolute_error: 0.8565\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0208 - mean_absolute_error: 0.7569 - val_loss: 1.4265 - val_mean_absolute_error: 0.8565\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0202 - mean_absolute_error: 0.7568 - val_loss: 1.4235 - val_mean_absolute_error: 0.8565\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0198 - mean_absolute_error: 0.7567 - val_loss: 1.4207 - val_mean_absolute_error: 0.8564\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0193 - mean_absolute_error: 0.7566 - val_loss: 1.4189 - val_mean_absolute_error: 0.8564\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0189 - mean_absolute_error: 0.7565 - val_loss: 1.4178 - val_mean_absolute_error: 0.8562\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0185 - mean_absolute_error: 0.7564 - val_loss: 1.4169 - val_mean_absolute_error: 0.8560\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0183 - mean_absolute_error: 0.7564 - val_loss: 1.4145 - val_mean_absolute_error: 0.8560\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0179 - mean_absolute_error: 0.7562 - val_loss: 1.4164 - val_mean_absolute_error: 0.8560\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0176 - mean_absolute_error: 0.7562 - val_loss: 1.4176 - val_mean_absolute_error: 0.8560\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0174 - mean_absolute_error: 0.7562 - val_loss: 1.4184 - val_mean_absolute_error: 0.8559\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0171 - mean_absolute_error: 0.7561 - val_loss: 1.4203 - val_mean_absolute_error: 0.8559\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0168 - mean_absolute_error: 0.7560 - val_loss: 1.4225 - val_mean_absolute_error: 0.8559\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0165 - mean_absolute_error: 0.7560 - val_loss: 1.4212 - val_mean_absolute_error: 0.8557\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0162 - mean_absolute_error: 0.7559 - val_loss: 1.4278 - val_mean_absolute_error: 0.8558\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0159 - mean_absolute_error: 0.7559 - val_loss: 1.4285 - val_mean_absolute_error: 0.8558\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0156 - mean_absolute_error: 0.7558 - val_loss: 1.4320 - val_mean_absolute_error: 0.8558\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0154 - mean_absolute_error: 0.7558 - val_loss: 1.4334 - val_mean_absolute_error: 0.8556\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0151 - mean_absolute_error: 0.7557 - val_loss: 1.4389 - val_mean_absolute_error: 0.8561\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0149 - mean_absolute_error: 0.7556 - val_loss: 1.4398 - val_mean_absolute_error: 0.8563\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0145 - mean_absolute_error: 0.7556 - val_loss: 1.4419 - val_mean_absolute_error: 0.8567\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0143 - mean_absolute_error: 0.7554 - val_loss: 1.4452 - val_mean_absolute_error: 0.8572\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0140 - mean_absolute_error: 0.7554 - val_loss: 1.4472 - val_mean_absolute_error: 0.8575\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0137 - mean_absolute_error: 0.7553 - val_loss: 1.4558 - val_mean_absolute_error: 0.8592\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0135 - mean_absolute_error: 0.7553 - val_loss: 1.4611 - val_mean_absolute_error: 0.8607\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0132 - mean_absolute_error: 0.7551 - val_loss: 1.4632 - val_mean_absolute_error: 0.8613\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0130 - mean_absolute_error: 0.7551 - val_loss: 1.4693 - val_mean_absolute_error: 0.8627\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0129 - mean_absolute_error: 0.7550 - val_loss: 1.4769 - val_mean_absolute_error: 0.8648\n",
      "Epoch 47/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0127 - mean_absolute_error: 0.7549 - val_loss: 1.4715 - val_mean_absolute_error: 0.8636\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0125 - mean_absolute_error: 0.7550 - val_loss: 1.4772 - val_mean_absolute_error: 0.8651\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0122 - mean_absolute_error: 0.7548 - val_loss: 1.4814 - val_mean_absolute_error: 0.8662\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0120 - mean_absolute_error: 0.7548 - val_loss: 1.4906 - val_mean_absolute_error: 0.8684\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0118 - mean_absolute_error: 0.7546 - val_loss: 1.4918 - val_mean_absolute_error: 0.8689\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0116 - mean_absolute_error: 0.7546 - val_loss: 1.4966 - val_mean_absolute_error: 0.8700\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0116 - mean_absolute_error: 0.7545 - val_loss: 1.4975 - val_mean_absolute_error: 0.8705\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0112 - mean_absolute_error: 0.7544 - val_loss: 1.5039 - val_mean_absolute_error: 0.8720\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0110 - mean_absolute_error: 0.7543 - val_loss: 1.5149 - val_mean_absolute_error: 0.8745\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0108 - mean_absolute_error: 0.7543 - val_loss: 1.5197 - val_mean_absolute_error: 0.8755\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0106 - mean_absolute_error: 0.7542 - val_loss: 1.5265 - val_mean_absolute_error: 0.8771\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0104 - mean_absolute_error: 0.7541 - val_loss: 1.5288 - val_mean_absolute_error: 0.8776\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0103 - mean_absolute_error: 0.7542 - val_loss: 1.5297 - val_mean_absolute_error: 0.8777\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0100 - mean_absolute_error: 0.7539 - val_loss: 1.5471 - val_mean_absolute_error: 0.8812\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0098 - mean_absolute_error: 0.7538 - val_loss: 1.5569 - val_mean_absolute_error: 0.8832\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0097 - mean_absolute_error: 0.7538 - val_loss: 1.5514 - val_mean_absolute_error: 0.8823\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0095 - mean_absolute_error: 0.7538 - val_loss: 1.5533 - val_mean_absolute_error: 0.8826\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0091 - mean_absolute_error: 0.7535 - val_loss: 1.5806 - val_mean_absolute_error: 0.8877\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0090 - mean_absolute_error: 0.7535 - val_loss: 1.5731 - val_mean_absolute_error: 0.8864\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0088 - mean_absolute_error: 0.7534 - val_loss: 1.5776 - val_mean_absolute_error: 0.8872\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0086 - mean_absolute_error: 0.7534 - val_loss: 1.5901 - val_mean_absolute_error: 0.8892\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0086 - mean_absolute_error: 0.7535 - val_loss: 1.5881 - val_mean_absolute_error: 0.8890\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0082 - mean_absolute_error: 0.7533 - val_loss: 1.5916 - val_mean_absolute_error: 0.8897\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 1.0081 - mean_absolute_error: 0.7532 - val_loss: 1.6065 - val_mean_absolute_error: 0.8920\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0079 - mean_absolute_error: 0.7532 - val_loss: 1.6094 - val_mean_absolute_error: 0.8929\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0078 - mean_absolute_error: 0.7531 - val_loss: 1.6132 - val_mean_absolute_error: 0.8934\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0078 - mean_absolute_error: 0.7530 - val_loss: 1.6297 - val_mean_absolute_error: 0.8955\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0077 - mean_absolute_error: 0.7532 - val_loss: 1.5891 - val_mean_absolute_error: 0.8905\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0073 - mean_absolute_error: 0.7528 - val_loss: 1.6076 - val_mean_absolute_error: 0.8931\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.0071 - mean_absolute_error: 0.7528 - val_loss: 1.6185 - val_mean_absolute_error: 0.8952\n"
     ]
    }
   ],
   "source": [
    " \n",
    "model = Sequential([\n",
    "    SimpleRNN(\n",
    "        units=1,\n",
    "        input_shape=(None, X_train_rnn.shape[1]),\n",
    "        kernel_regularizer=regularizers.L2(0.01),\n",
    "        recurrent_regularizer=regularizers.L2(0.01),\n",
    "        activation='relu'\n",
    "        )\n",
    "        ])\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/rnn_1_flights_ontime/\n",
    "\n",
    "log_dir = \"logs/rnn_1_flights_ontime/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_rnn,\n",
    "    epochs=100,\n",
    "    validation_data=val_rnn,\n",
    "    callbacks=[tensorboard_callback, early_stopping]\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 9089), started 0:00:24 ago. (Use '!kill 9089' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-56e599d88e90e8b0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-56e599d88e90e8b0\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !kill 220\n",
    "%tensorboard --logdir logs/rnn_1_flights_ontime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using a shallow RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 48s]\n",
      "val_loss: 1.3270539045333862\n",
      "\n",
      "Best val_loss So Far: 1.239393711090088\n",
      "Total elapsed time: 00h 01m 11s\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int('n_hidden', min_value=1, max_value=2, default=2)\n",
    "    n_neurons = hp.Int('n_neurons', min_value=1, max_value=32, default=16)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, default=0.0)\n",
    "    recurrent_dropout_rate = hp.Float('recurrent_dropout_rate', min_value=0.0, max_value=0.5, default=0.0)\n",
    "    kernel_reg = hp.Float('kernel_reg', min_value=1e-4, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "    recurr_reg = hp.Float('recurr_reg', min_value=1e-4, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer with dropout\n",
    "    model.add(Dropout(dropout_rate, \n",
    "                      input_shape=(None, X_train_rnn.shape[1])))\n",
    "                     \n",
    "    # model.add(SimpleRNN(units=n_neurons, \n",
    "    #                     input_shape=(None, X_train_rnn.shape[1]), \n",
    "    #                     activation='relu', \n",
    "    #                     return_sequences=True, \n",
    "    #                     kernel_regularizer=L2(kernel_reg),\n",
    "    #                     recurrent_regularizer=L2(recurr_reg))\n",
    "\n",
    "    # First n-1 Hidden layers\n",
    "    for _ in range(n_hidden-1):\n",
    "        model.add(SimpleRNN(units=n_neurons, \n",
    "                            activation='relu', \n",
    "                            return_sequences=True,\n",
    "                            kernel_regularizer=L2(kernel_reg),\n",
    "                            recurrent_regularizer=L2(recurr_reg),\n",
    "                            dropout = dropout_rate,\n",
    "                            recurrent_dropout = recurrent_dropout_rate))\n",
    "\n",
    "    # Last hidden layer\n",
    "    if n_hidden > 0:\n",
    "        model.add(SimpleRNN(units=n_neurons, \n",
    "                            activation='relu', \n",
    "                            kernel_regularizer=L2(kernel_reg),\n",
    "                            recurrent_regularizer=L2(recurr_reg),\n",
    "                            dropout = dropout_rate,\n",
    "                            recurrent_dropout = recurrent_dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                  loss='mean_squared_error', \n",
    "                  metrics=['mean_absolute_error'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#     build_model,\n",
    "#     objective='val_loss',\n",
    "#     max_trials=50,\n",
    "#     num_initial_points=2,\n",
    "#     overwrite=True\n",
    "# )\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(train_rnn, epochs=500, validation_data=val_rnn, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 3 hyperparameter sets:\n",
      "{'n_hidden': 2, 'n_neurons': 28, 'learning_rate': 0.0038915899689643918, 'dropout_rate': 0.1347496912624052, 'recurrent_dropout_rate': 0.011123302279362424, 'kernel_reg': 0.004347669530569911, 'recurr_reg': 0.019211177831550998}\n",
      "{'n_hidden': 1, 'n_neurons': 1, 'learning_rate': 0.0002590015503494759, 'dropout_rate': 0.38334649514249525, 'recurrent_dropout_rate': 0.10174476464789312, 'kernel_reg': 0.0005400562198482222, 'recurr_reg': 0.011842609343404916}\n",
      "{'n_hidden': 1, 'n_neurons': 15, 'learning_rate': 0.0014267123289125915, 'dropout_rate': 0.273838857755164, 'recurrent_dropout_rate': 0.42722199443089204, 'kernel_reg': 0.0007982790969186325, 'recurr_reg': 0.08684008944452379} \n",
      "\n",
      "Trial 1 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 28\n",
      "learning_rate: 0.0038915899689643918\n",
      "dropout_rate: 0.1347496912624052\n",
      "recurrent_dropout_rate: 0.011123302279362424\n",
      "kernel_reg: 0.004347669530569911\n",
      "recurr_reg: 0.019211177831550998\n",
      "Score: 1.239393711090088\n",
      "\n",
      "Best trial validation loss 1.239393711090088\n",
      "Best trial validation MAE 0.7535279393196106\n"
     ]
    }
   ],
   "source": [
    "best_3hps = tuner.get_best_hyperparameters(num_trials=3)\n",
    "\n",
    "print(\"Best 3 hyperparameter sets:\")\n",
    "print(best_3hps[0].values)\n",
    "print(best_3hps[1].values)\n",
    "print(best_3hps[2].values, '\\n')\n",
    "\n",
    "\n",
    "best_trial = tuner.oracle.get_best_trials(1)[0]\n",
    "\n",
    "best_trial.summary()\n",
    "\n",
    "print(\"\\nBest trial validation loss\", best_trial.metrics.get_last_value('val_loss'))\n",
    "print(\"Best trial validation MAE\", best_trial.metrics.get_last_value('mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. add TensorBoard to RNN\n",
    "2. Add L1 and L2 regularization to Keras Tuner \n",
    "3. Tune for 500 trials (overnight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prediction-rTmYhf-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
