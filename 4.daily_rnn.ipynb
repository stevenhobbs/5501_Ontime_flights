{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Sequential, regularizers\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Embedding, LSTM, GRU\n",
    "from tensorflow.keras.optimizers.legacy import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.utils import timeseries_dataset_from_array\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "\n",
    "from keras.regularizers import L1, L2, L1L2\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data & column groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_DATA_PATH = \"data.v3/daily\" \n",
    "\n",
    "df = pd.read_parquet(os.path.join(DAILY_DATA_PATH, \"daily_flights_and_weather_merged.parquet\"))\n",
    "\n",
    "# Flights column groups\n",
    "flights_terminal_cols = ['flights_arr_A', 'flights_arr_B', 'flights_arr_C', 'flights_arr_D', 'flights_arr_E',\n",
    "                         'flights_dep_A', 'flights_dep_B', 'flights_dep_C', 'flights_dep_D', 'flights_dep_E']\n",
    "\n",
    "flights_non_terminal_cols = ['flights_total', 'flights_cancel', 'flights_delay', 'flights_ontime',\n",
    "                             'flights_arr_ontime', 'flights_arr_delay', 'flights_arr_cancel',\n",
    "                             'flights_dep_ontime', 'flights_dep_delay', 'flights_dep_cancel']\n",
    "\n",
    "flights_percentage_cols = ['flights_cancel_pct', 'flights_delay_pct', 'flights_ontime_pct',\n",
    "                            'flights_arr_delay_pct', 'flights_arr_ontime_pct', 'flights_arr_cancel_pct',\n",
    "                            'flights_dep_delay_pct', 'flights_dep_ontime_pct', 'flights_dep_cancel_pct']\n",
    "\n",
    "# Date column groups\n",
    "date_cols = ['date', 'covid', 'ordinal_date', 'year', 'month', 'day_of_month', 'day_of_week', 'season', 'holiday', 'halloween', 'xmas_eve', 'new_years_eve', 'jan_2', 'jan_3', 'day_before_easter', 'days_until_xmas', 'days_until_thanksgiving', 'days_until_july_4th', 'days_until_labor_day', 'days_until_memorial_day']\n",
    "\n",
    "# Weather column groups\n",
    "weather_cols = ['wx_temperature_max', 'wx_temperature_min', 'wx_apcp', 'wx_prate', 'wx_asnow', 'wx_frozr', 'wx_vis', 'wx_gust', 'wx_maxref', 'wx_cape', 'wx_lftx', 'wx_wind_speed', 'wx_wind_direction']\n",
    "\n",
    "# Lag column groups\n",
    "lag_cols =  ['flights_total_lag_1', 'flights_total_lag_2', 'flights_total_lag_3', 'flights_total_lag_4', 'flights_total_lag_5', 'flights_total_lag_6', 'flights_total_lag_7', 'flights_cancel_lag_1', 'flights_cancel_lag_2', 'flights_cancel_lag_3', 'flights_cancel_lag_4', 'flights_cancel_lag_5', 'flights_cancel_lag_6', 'flights_cancel_lag_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['random', 'covid', 'ordinal_date', 'year', 'month', 'day_of_month', 'day_of_week', 'season', 'holiday', 'halloween', 'xmas_eve', 'new_years_eve', 'jan_2', 'jan_3', 'day_before_easter', 'days_until_xmas', 'days_until_thanksgiving', 'days_until_july_4th', 'days_until_labor_day', 'days_until_memorial_day', 'wx_temperature_max', 'wx_temperature_min', 'wx_apcp', 'wx_prate', 'wx_asnow', 'wx_frozr', 'wx_vis', 'wx_gust', 'wx_maxref', 'wx_cape', 'wx_lftx', 'wx_wind_speed', 'wx_wind_direction', 'flights_total_lag_1', 'flights_total_lag_2', 'flights_total_lag_3', 'flights_total_lag_4', 'flights_total_lag_5', 'flights_total_lag_6', 'flights_total_lag_7', 'flights_cancel_lag_1', 'flights_cancel_lag_2', 'flights_cancel_lag_3', 'flights_cancel_lag_4', 'flights_cancel_lag_5', 'flights_cancel_lag_6', 'flights_cancel_lag_7']\n",
      "Target columns: ['flights_total', 'flights_cancel', 'flights_delay', 'flights_ontime', 'flights_arr_ontime', 'flights_arr_delay', 'flights_arr_cancel', 'flights_dep_ontime', 'flights_dep_delay', 'flights_dep_cancel', 'flights_cancel_pct', 'flights_delay_pct', 'flights_ontime_pct', 'flights_arr_delay_pct', 'flights_arr_ontime_pct', 'flights_arr_cancel_pct', 'flights_dep_delay_pct', 'flights_dep_ontime_pct', 'flights_dep_cancel_pct']\n",
      "\n",
      "Unique data types in X\n",
      "float64    23\n",
      "object     11\n",
      "int64       7\n",
      "float32     4\n",
      "int32       2\n",
      "Name: count, dtype: int64\n",
      "Categorical columns to one-hot-encode: ['covid', 'month', 'day_of_week', 'season', 'holiday', 'halloween', 'xmas_eve', 'new_years_eve', 'jan_2', 'jan_3', 'day_before_easter']\n"
     ]
    }
   ],
   "source": [
    "# Select features and targets\n",
    "train_features = ['random'] + date_cols + weather_cols + lag_cols\n",
    "targets = flights_non_terminal_cols + flights_percentage_cols\n",
    "\n",
    "# Create X and y\n",
    "X = df[train_features].drop('date', axis=1)\n",
    "y = df[targets]\n",
    "\n",
    "print(f\"Feature names: {X.columns.tolist()}\")\n",
    "print(f\"Target columns: {y.columns.tolist()}\", end=\"\\n\\n\")\n",
    "print(\"Unique data types in X\", X.dtypes.value_counts(), sep = '\\n')\n",
    "\n",
    "# Identify categorical and numeric columns in X\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include = ['float64', 'float32', 'int32', 'int64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns to one-hot-encode: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split - \"flights_ontime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_full shape: (1516, 47)\n",
      "y_train_full shape: (1516,)\n",
      "X_train shape: (1364, 47)\n",
      "y_train shape: (1364,)\n",
      "X_Test shape: (169, 47)\n",
      "y_Test shape: (169,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y['flights_ontime'], test_size=0.1, random_state=42)\n",
    "\n",
    "# Split data into X_train_rull and y_train_full into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print shapes\n",
    "print(\"X_train_full shape:\", X_train_full.shape)\n",
    "print(\"y_train_full shape:\", y_train_full.shape)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "print(\"X_Test shape:\", X_test.shape)\n",
    "print(\"y_Test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DENSE NETWORK PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit transformers to the training data\n",
    "f_scaler = StandardScaler()\n",
    "f_scaler.fit(X_train[numeric_cols])\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Some observed holidays may not be in the training data\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "t_scaler = StandardScaler()\n",
    "t_scaler.fit(y_train.values.reshape(-1, 1)) # reshape y_train to be 2D\n",
    "\n",
    "# Define preprocessor\n",
    "def preprocess(features, target, set_global_scaler = False):\n",
    "    global global_targer_scaler\n",
    "\n",
    "    scaled_features = f_scaler.transform(features[numeric_cols])\n",
    "    encoded_features = ohe.transform(features[categorical_cols])\n",
    "    scaled_target = t_scaler.transform(target.values.reshape(-1, 1))\n",
    "    processed_features = np.concatenate([scaled_features, encoded_features], axis=1)\n",
    "\n",
    "    if set_global_scaler:\n",
    "        global_targer_scaler = t_scaler\n",
    "\n",
    "    return processed_features, scaled_target\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_d, y_train_d = preprocess(X_train, y_train, set_global_scaler=True)\n",
    "X_val_d, y_val_d = preprocess(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT `flights_ontime` WITH 1 NEURON\n",
    "\n",
    "The goal of this section is to simulate linear regression using a neural newtork with one neuron and no activation function. We'll use L2 regularization to simulate ridge regression and compare results to those from Sklearn's lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow datasets (not timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow datasets\n",
    "train_ds_flights_ontime_d = Dataset.from_tensor_slices((X_train_d, y_train_d)).shuffle(len(X_train_d))\n",
    "val_ds_flights_ontime_d = Dataset.from_tensor_slices((X_val_d, y_val_d)).shuffle(len(X_val_d))\n",
    "\n",
    "# Batch and prefetch\n",
    "batch_size = 32\n",
    "train_ds_flights_ontime_d = train_ds_flights_ontime_d.batch(batch_size).prefetch(AUTOTUNE)\n",
    "val_ds_flights_ontime_d = val_ds_flights_ontime_d.batch(batch_size).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create R-squared metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    y_true_inv = tf.numpy_function(global_targer_scaler.inverse_transform, [y_true], tf.float32)\n",
    "    y_pred_inv = tf.numpy_function(global_targer_scaler.inverse_transform, [y_pred], tf.float32)\n",
    "    SS_res =  K.sum(K.square(y_true_inv - y_pred_inv)) \n",
    "    SS_tot = K.sum(K.square(y_true_inv - K.mean(y_true_inv))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Neuron \"linear regression\" Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 81 Complete [00h 00m 05s]\n",
      "val_loss: 0.5952489674091339\n",
      "\n",
      "Best val_loss So Far: 0.4128335863351822\n",
      "Total elapsed time: 00h 11m 25s\n",
      "\n",
      "Search: Running Trial #82\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.00017155        |0.0080984         |learning_rate\n",
      "0.06706           |5.9607e-05        |l1_regularization\n",
      "4.1022e-05        |0.00017501        |l2_regularization\n",
      "\n",
      "Epoch 1/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 3.7233 - mean_absolute_error: 1.4221 - r_squared: -2.3210 - val_loss: 4.6178 - val_mean_absolute_error: 1.5305 - val_r_squared: -2.0682\n",
      "Epoch 2/500\n",
      "43/43 [==============================] - 0s 727us/step - loss: 3.4768 - mean_absolute_error: 1.3599 - r_squared: -1.9945 - val_loss: 4.3655 - val_mean_absolute_error: 1.4700 - val_r_squared: -2.0890\n",
      "Epoch 3/500\n",
      "43/43 [==============================] - 0s 665us/step - loss: 3.2533 - mean_absolute_error: 1.3028 - r_squared: -1.8430 - val_loss: 4.1239 - val_mean_absolute_error: 1.4129 - val_r_squared: -1.6964\n",
      "Epoch 4/500\n",
      "43/43 [==============================] - 0s 673us/step - loss: 3.0512 - mean_absolute_error: 1.2486 - r_squared: -1.6070 - val_loss: 3.9023 - val_mean_absolute_error: 1.3594 - val_r_squared: -1.5550\n",
      "Epoch 5/500\n",
      "43/43 [==============================] - 0s 662us/step - loss: 2.8658 - mean_absolute_error: 1.1985 - r_squared: -1.3945 - val_loss: 3.7014 - val_mean_absolute_error: 1.3106 - val_r_squared: -1.3323\n",
      "Epoch 6/500\n",
      "43/43 [==============================] - 0s 667us/step - loss: 2.6974 - mean_absolute_error: 1.1511 - r_squared: -1.2105 - val_loss: 3.5181 - val_mean_absolute_error: 1.2653 - val_r_squared: -1.5288\n",
      "Epoch 7/500\n",
      "43/43 [==============================] - 0s 659us/step - loss: 2.5446 - mean_absolute_error: 1.1071 - r_squared: -1.0076 - val_loss: 3.3435 - val_mean_absolute_error: 1.2242 - val_r_squared: -1.2030\n",
      "Epoch 8/500\n",
      "43/43 [==============================] - 0s 655us/step - loss: 2.4040 - mean_absolute_error: 1.0662 - r_squared: -0.9230 - val_loss: 3.1869 - val_mean_absolute_error: 1.1853 - val_r_squared: -1.0966\n",
      "Epoch 9/500\n",
      "43/43 [==============================] - 0s 658us/step - loss: 2.2755 - mean_absolute_error: 1.0283 - r_squared: -0.7491 - val_loss: 3.0421 - val_mean_absolute_error: 1.1494 - val_r_squared: -0.8985\n",
      "Epoch 10/500\n",
      "43/43 [==============================] - 0s 651us/step - loss: 2.1583 - mean_absolute_error: 0.9931 - r_squared: -0.6788 - val_loss: 2.9013 - val_mean_absolute_error: 1.1143 - val_r_squared: -0.8149\n",
      "Epoch 11/500\n",
      "43/43 [==============================] - 0s 677us/step - loss: 2.0484 - mean_absolute_error: 0.9598 - r_squared: -0.5244 - val_loss: 2.7741 - val_mean_absolute_error: 1.0815 - val_r_squared: -0.6927\n",
      "Epoch 12/500\n",
      "43/43 [==============================] - 0s 720us/step - loss: 1.9475 - mean_absolute_error: 0.9290 - r_squared: -0.4563 - val_loss: 2.6575 - val_mean_absolute_error: 1.0505 - val_r_squared: -0.6981\n",
      "Epoch 13/500\n",
      "43/43 [==============================] - 0s 747us/step - loss: 1.8543 - mean_absolute_error: 0.9007 - r_squared: -0.3494 - val_loss: 2.5438 - val_mean_absolute_error: 1.0202 - val_r_squared: -0.5548\n",
      "Epoch 14/500\n",
      "43/43 [==============================] - 0s 692us/step - loss: 1.7673 - mean_absolute_error: 0.8739 - r_squared: -0.2948 - val_loss: 2.4378 - val_mean_absolute_error: 0.9918 - val_r_squared: -0.4220\n",
      "Epoch 15/500\n",
      "43/43 [==============================] - 0s 662us/step - loss: 1.6879 - mean_absolute_error: 0.8483 - r_squared: -0.2230 - val_loss: 2.3432 - val_mean_absolute_error: 0.9646 - val_r_squared: -0.4212\n",
      "Epoch 16/500\n",
      "43/43 [==============================] - 0s 641us/step - loss: 1.6152 - mean_absolute_error: 0.8244 - r_squared: -0.1658 - val_loss: 2.2488 - val_mean_absolute_error: 0.9386 - val_r_squared: -0.3670\n",
      "Epoch 17/500\n",
      "43/43 [==============================] - 0s 718us/step - loss: 1.5468 - mean_absolute_error: 0.8026 - r_squared: -0.1202 - val_loss: 2.1615 - val_mean_absolute_error: 0.9148 - val_r_squared: -0.2534\n",
      "Epoch 18/500\n",
      "43/43 [==============================] - 0s 676us/step - loss: 1.4839 - mean_absolute_error: 0.7814 - r_squared: -0.0755 - val_loss: 2.0859 - val_mean_absolute_error: 0.8917 - val_r_squared: -0.3277\n",
      "Epoch 19/500\n",
      "43/43 [==============================] - 0s 687us/step - loss: 1.4259 - mean_absolute_error: 0.7622 - r_squared: -0.0334 - val_loss: 2.0081 - val_mean_absolute_error: 0.8690 - val_r_squared: -0.2412\n",
      "Epoch 20/500\n",
      "43/43 [==============================] - 0s 635us/step - loss: 1.3713 - mean_absolute_error: 0.7434 - r_squared: 0.0218 - val_loss: 1.9410 - val_mean_absolute_error: 0.8476 - val_r_squared: -0.0799\n",
      "Epoch 21/500\n",
      "43/43 [==============================] - 0s 691us/step - loss: 1.3209 - mean_absolute_error: 0.7268 - r_squared: 0.0020 - val_loss: 1.8723 - val_mean_absolute_error: 0.8272 - val_r_squared: -0.1519\n",
      "Epoch 22/500\n",
      "43/43 [==============================] - 0s 669us/step - loss: 1.2729 - mean_absolute_error: 0.7100 - r_squared: 0.1099 - val_loss: 1.8132 - val_mean_absolute_error: 0.8073 - val_r_squared: -0.0626\n",
      "Epoch 23/500\n",
      "43/43 [==============================] - 0s 658us/step - loss: 1.2287 - mean_absolute_error: 0.6944 - r_squared: 0.1535 - val_loss: 1.7534 - val_mean_absolute_error: 0.7888 - val_r_squared: -0.0366\n",
      "Epoch 24/500\n",
      "43/43 [==============================] - 0s 643us/step - loss: 1.1881 - mean_absolute_error: 0.6804 - r_squared: 0.1611 - val_loss: 1.6959 - val_mean_absolute_error: 0.7723 - val_r_squared: -0.0020\n",
      "Epoch 25/500\n",
      "43/43 [==============================] - 0s 690us/step - loss: 1.1500 - mean_absolute_error: 0.6667 - r_squared: 0.1986 - val_loss: 1.6449 - val_mean_absolute_error: 0.7563 - val_r_squared: 0.0932\n",
      "Epoch 26/500\n",
      "43/43 [==============================] - 0s 655us/step - loss: 1.1148 - mean_absolute_error: 0.6539 - r_squared: 0.2335 - val_loss: 1.5961 - val_mean_absolute_error: 0.7407 - val_r_squared: 0.0635\n",
      "Epoch 27/500\n",
      "43/43 [==============================] - 0s 732us/step - loss: 1.0813 - mean_absolute_error: 0.6427 - r_squared: 0.2567 - val_loss: 1.5484 - val_mean_absolute_error: 0.7269 - val_r_squared: 0.0952\n",
      "Epoch 28/500\n",
      "43/43 [==============================] - 0s 748us/step - loss: 1.0499 - mean_absolute_error: 0.6318 - r_squared: 0.2840 - val_loss: 1.5099 - val_mean_absolute_error: 0.7142 - val_r_squared: 0.0079\n",
      "Epoch 29/500\n",
      "43/43 [==============================] - 0s 676us/step - loss: 1.0209 - mean_absolute_error: 0.6216 - r_squared: 0.2651 - val_loss: 1.4622 - val_mean_absolute_error: 0.7018 - val_r_squared: 0.1257\n",
      "Epoch 30/500\n",
      "43/43 [==============================] - 0s 704us/step - loss: 0.9935 - mean_absolute_error: 0.6119 - r_squared: 0.3154 - val_loss: 1.4237 - val_mean_absolute_error: 0.6908 - val_r_squared: 0.1356\n",
      "Epoch 31/500\n",
      "43/43 [==============================] - 0s 680us/step - loss: 0.9683 - mean_absolute_error: 0.6029 - r_squared: 0.3129 - val_loss: 1.3875 - val_mean_absolute_error: 0.6811 - val_r_squared: 0.1741\n",
      "Epoch 32/500\n",
      "43/43 [==============================] - 0s 658us/step - loss: 0.9454 - mean_absolute_error: 0.5947 - r_squared: 0.3301 - val_loss: 1.3547 - val_mean_absolute_error: 0.6712 - val_r_squared: 0.1222\n",
      "Epoch 33/500\n",
      "43/43 [==============================] - 0s 658us/step - loss: 0.9243 - mean_absolute_error: 0.5874 - r_squared: 0.3655 - val_loss: 1.3288 - val_mean_absolute_error: 0.6630 - val_r_squared: 0.2211\n",
      "Epoch 34/500\n",
      "43/43 [==============================] - 0s 666us/step - loss: 0.9049 - mean_absolute_error: 0.5805 - r_squared: 0.3737 - val_loss: 1.2981 - val_mean_absolute_error: 0.6545 - val_r_squared: 0.1632\n",
      "Epoch 35/500\n",
      "43/43 [==============================] - 0s 658us/step - loss: 0.8870 - mean_absolute_error: 0.5740 - r_squared: 0.3858 - val_loss: 1.2723 - val_mean_absolute_error: 0.6460 - val_r_squared: 0.2329\n",
      "Epoch 36/500\n",
      "43/43 [==============================] - 0s 668us/step - loss: 0.8705 - mean_absolute_error: 0.5686 - r_squared: 0.3957 - val_loss: 1.2424 - val_mean_absolute_error: 0.6385 - val_r_squared: 0.2590\n",
      "Epoch 37/500\n",
      "43/43 [==============================] - 0s 732us/step - loss: 0.8552 - mean_absolute_error: 0.5631 - r_squared: 0.4059 - val_loss: 1.2190 - val_mean_absolute_error: 0.6309 - val_r_squared: 0.2655\n",
      "Epoch 38/500\n",
      "43/43 [==============================] - 0s 686us/step - loss: 0.8412 - mean_absolute_error: 0.5581 - r_squared: 0.3900 - val_loss: 1.1958 - val_mean_absolute_error: 0.6236 - val_r_squared: 0.3133\n",
      "Epoch 39/500\n",
      "43/43 [==============================] - 0s 656us/step - loss: 0.8282 - mean_absolute_error: 0.5535 - r_squared: 0.4166 - val_loss: 1.1765 - val_mean_absolute_error: 0.6179 - val_r_squared: 0.2703\n",
      "Epoch 40/500\n",
      "43/43 [==============================] - 0s 644us/step - loss: 0.8162 - mean_absolute_error: 0.5492 - r_squared: 0.4067 - val_loss: 1.1552 - val_mean_absolute_error: 0.6117 - val_r_squared: 0.2930\n",
      "Epoch 41/500\n",
      "43/43 [==============================] - 0s 690us/step - loss: 0.8052 - mean_absolute_error: 0.5451 - r_squared: 0.4311 - val_loss: 1.1378 - val_mean_absolute_error: 0.6070 - val_r_squared: 0.3521\n",
      "Epoch 42/500\n",
      "43/43 [==============================] - 0s 648us/step - loss: 0.7948 - mean_absolute_error: 0.5416 - r_squared: 0.4364 - val_loss: 1.1188 - val_mean_absolute_error: 0.6015 - val_r_squared: 0.2660\n",
      "Epoch 43/500\n",
      "43/43 [==============================] - 0s 664us/step - loss: 0.7848 - mean_absolute_error: 0.5385 - r_squared: 0.4303 - val_loss: 1.1023 - val_mean_absolute_error: 0.5974 - val_r_squared: 0.3419\n",
      "Epoch 44/500\n",
      "43/43 [==============================] - 0s 647us/step - loss: 0.7756 - mean_absolute_error: 0.5356 - r_squared: 0.4458 - val_loss: 1.0875 - val_mean_absolute_error: 0.5936 - val_r_squared: 0.3444\n",
      "Epoch 45/500\n",
      "43/43 [==============================] - 0s 699us/step - loss: 0.7664 - mean_absolute_error: 0.5324 - r_squared: 0.4462 - val_loss: 1.0724 - val_mean_absolute_error: 0.5895 - val_r_squared: 0.2398\n",
      "Epoch 46/500\n",
      "43/43 [==============================] - 0s 672us/step - loss: 0.7575 - mean_absolute_error: 0.5302 - r_squared: 0.4541 - val_loss: 1.0584 - val_mean_absolute_error: 0.5865 - val_r_squared: 0.3292\n",
      "Epoch 47/500\n",
      "43/43 [==============================] - 0s 688us/step - loss: 0.7493 - mean_absolute_error: 0.5275 - r_squared: 0.4522 - val_loss: 1.0435 - val_mean_absolute_error: 0.5825 - val_r_squared: 0.3934\n",
      "Epoch 48/500\n",
      "43/43 [==============================] - 0s 739us/step - loss: 0.7411 - mean_absolute_error: 0.5252 - r_squared: 0.4663 - val_loss: 1.0263 - val_mean_absolute_error: 0.5791 - val_r_squared: 0.3347\n",
      "Epoch 49/500\n",
      "43/43 [==============================] - 0s 713us/step - loss: 0.7331 - mean_absolute_error: 0.5232 - r_squared: 0.4772 - val_loss: 1.0158 - val_mean_absolute_error: 0.5765 - val_r_squared: 0.3249\n",
      "Epoch 50/500\n",
      "43/43 [==============================] - 0s 686us/step - loss: 0.7255 - mean_absolute_error: 0.5216 - r_squared: 0.4605 - val_loss: 1.0002 - val_mean_absolute_error: 0.5737 - val_r_squared: 0.4277\n",
      "Epoch 51/500\n",
      "43/43 [==============================] - 0s 690us/step - loss: 0.7178 - mean_absolute_error: 0.5197 - r_squared: 0.4669 - val_loss: 0.9888 - val_mean_absolute_error: 0.5710 - val_r_squared: 0.3935\n",
      "Epoch 52/500\n",
      "43/43 [==============================] - 0s 665us/step - loss: 0.7105 - mean_absolute_error: 0.5180 - r_squared: 0.4727 - val_loss: 0.9761 - val_mean_absolute_error: 0.5682 - val_r_squared: 0.3860\n",
      "Epoch 53/500\n",
      "43/43 [==============================] - 0s 639us/step - loss: 0.7037 - mean_absolute_error: 0.5165 - r_squared: 0.4804 - val_loss: 0.9625 - val_mean_absolute_error: 0.5660 - val_r_squared: 0.4108\n",
      "Epoch 54/500\n",
      "43/43 [==============================] - 0s 695us/step - loss: 0.6971 - mean_absolute_error: 0.5154 - r_squared: 0.4781 - val_loss: 0.9526 - val_mean_absolute_error: 0.5636 - val_r_squared: 0.3985\n",
      "Epoch 55/500\n",
      "43/43 [==============================] - 0s 713us/step - loss: 0.6906 - mean_absolute_error: 0.5141 - r_squared: 0.4923 - val_loss: 0.9371 - val_mean_absolute_error: 0.5608 - val_r_squared: 0.4347\n",
      "Epoch 56/500\n",
      "43/43 [==============================] - 0s 766us/step - loss: 0.6841 - mean_absolute_error: 0.5130 - r_squared: 0.5041 - val_loss: 0.9268 - val_mean_absolute_error: 0.5592 - val_r_squared: 0.4215\n",
      "Epoch 57/500\n",
      "43/43 [==============================] - 0s 990us/step - loss: 0.6778 - mean_absolute_error: 0.5118 - r_squared: 0.4668 - val_loss: 0.9152 - val_mean_absolute_error: 0.5569 - val_r_squared: 0.4015\n",
      "Epoch 58/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.6716 - mean_absolute_error: 0.5105 - r_squared: 0.4932 - val_loss: 0.9042 - val_mean_absolute_error: 0.5547 - val_r_squared: 0.4613\n",
      "Epoch 59/500\n",
      "43/43 [==============================] - 0s 756us/step - loss: 0.6658 - mean_absolute_error: 0.5092 - r_squared: 0.4773 - val_loss: 0.8927 - val_mean_absolute_error: 0.5528 - val_r_squared: 0.4411\n",
      "Epoch 60/500\n",
      "43/43 [==============================] - 0s 699us/step - loss: 0.6598 - mean_absolute_error: 0.5084 - r_squared: 0.4936 - val_loss: 0.8814 - val_mean_absolute_error: 0.5512 - val_r_squared: 0.4211\n",
      "Epoch 61/500\n",
      "43/43 [==============================] - 0s 666us/step - loss: 0.6543 - mean_absolute_error: 0.5070 - r_squared: 0.4938 - val_loss: 0.8704 - val_mean_absolute_error: 0.5483 - val_r_squared: 0.4265\n",
      "Epoch 62/500\n",
      "43/43 [==============================] - 0s 672us/step - loss: 0.6490 - mean_absolute_error: 0.5062 - r_squared: 0.4905 - val_loss: 0.8594 - val_mean_absolute_error: 0.5464 - val_r_squared: 0.4279\n",
      "Epoch 63/500\n",
      "43/43 [==============================] - 0s 660us/step - loss: 0.6437 - mean_absolute_error: 0.5055 - r_squared: 0.5051 - val_loss: 0.8516 - val_mean_absolute_error: 0.5449 - val_r_squared: 0.4304\n",
      "Epoch 64/500\n",
      "43/43 [==============================] - 0s 651us/step - loss: 0.6387 - mean_absolute_error: 0.5047 - r_squared: 0.4876 - val_loss: 0.8417 - val_mean_absolute_error: 0.5434 - val_r_squared: 0.4625\n",
      "Epoch 65/500\n",
      "43/43 [==============================] - 0s 666us/step - loss: 0.6342 - mean_absolute_error: 0.5039 - r_squared: 0.5162 - val_loss: 0.8314 - val_mean_absolute_error: 0.5417 - val_r_squared: 0.4489\n",
      "Epoch 66/500\n",
      "43/43 [==============================] - 0s 686us/step - loss: 0.6296 - mean_absolute_error: 0.5030 - r_squared: 0.5161 - val_loss: 0.8217 - val_mean_absolute_error: 0.5399 - val_r_squared: 0.4723\n",
      "Epoch 67/500\n",
      "43/43 [==============================] - 0s 699us/step - loss: 0.6253 - mean_absolute_error: 0.5022 - r_squared: 0.5104 - val_loss: 0.8132 - val_mean_absolute_error: 0.5382 - val_r_squared: 0.4835\n",
      "Epoch 68/500\n",
      "43/43 [==============================] - 0s 729us/step - loss: 0.6210 - mean_absolute_error: 0.5014 - r_squared: 0.5232 - val_loss: 0.8040 - val_mean_absolute_error: 0.5364 - val_r_squared: 0.4759\n",
      "Epoch 69/500\n",
      "43/43 [==============================] - 0s 741us/step - loss: 0.6170 - mean_absolute_error: 0.5007 - r_squared: 0.4950 - val_loss: 0.7978 - val_mean_absolute_error: 0.5357 - val_r_squared: 0.4929\n",
      "Epoch 70/500\n",
      "43/43 [==============================] - 0s 753us/step - loss: 0.6130 - mean_absolute_error: 0.4999 - r_squared: 0.5126 - val_loss: 0.7873 - val_mean_absolute_error: 0.5339 - val_r_squared: 0.4359\n",
      "Epoch 71/500\n",
      "43/43 [==============================] - 0s 720us/step - loss: 0.6093 - mean_absolute_error: 0.4995 - r_squared: 0.4945 - val_loss: 0.7788 - val_mean_absolute_error: 0.5331 - val_r_squared: 0.4718\n",
      "Epoch 72/500\n",
      "43/43 [==============================] - 0s 697us/step - loss: 0.6054 - mean_absolute_error: 0.4991 - r_squared: 0.5067 - val_loss: 0.7716 - val_mean_absolute_error: 0.5320 - val_r_squared: 0.4594\n",
      "Epoch 73/500\n",
      "43/43 [==============================] - 0s 697us/step - loss: 0.6017 - mean_absolute_error: 0.4989 - r_squared: 0.4916 - val_loss: 0.7624 - val_mean_absolute_error: 0.5311 - val_r_squared: 0.4997\n",
      "Epoch 74/500\n",
      "43/43 [==============================] - 0s 634us/step - loss: 0.5984 - mean_absolute_error: 0.4987 - r_squared: 0.5137 - val_loss: 0.7559 - val_mean_absolute_error: 0.5302 - val_r_squared: 0.5216\n",
      "Epoch 75/500\n",
      "43/43 [==============================] - 0s 696us/step - loss: 0.5946 - mean_absolute_error: 0.4976 - r_squared: 0.5178 - val_loss: 0.7485 - val_mean_absolute_error: 0.5293 - val_r_squared: 0.5057\n",
      "Epoch 76/500\n",
      "43/43 [==============================] - 0s 669us/step - loss: 0.5912 - mean_absolute_error: 0.4976 - r_squared: 0.5192 - val_loss: 0.7411 - val_mean_absolute_error: 0.5288 - val_r_squared: 0.4953\n",
      "Epoch 77/500\n",
      "43/43 [==============================] - 0s 688us/step - loss: 0.5880 - mean_absolute_error: 0.4972 - r_squared: 0.5194 - val_loss: 0.7336 - val_mean_absolute_error: 0.5279 - val_r_squared: 0.5244\n",
      "Epoch 78/500\n",
      "43/43 [==============================] - 0s 721us/step - loss: 0.5846 - mean_absolute_error: 0.4968 - r_squared: 0.5268 - val_loss: 0.7258 - val_mean_absolute_error: 0.5270 - val_r_squared: 0.5504\n",
      "Epoch 79/500\n",
      "43/43 [==============================] - 0s 713us/step - loss: 0.5813 - mean_absolute_error: 0.4965 - r_squared: 0.5277 - val_loss: 0.7199 - val_mean_absolute_error: 0.5266 - val_r_squared: 0.5060\n",
      "Epoch 80/500\n",
      "43/43 [==============================] - 0s 705us/step - loss: 0.5783 - mean_absolute_error: 0.4966 - r_squared: 0.5284 - val_loss: 0.7126 - val_mean_absolute_error: 0.5262 - val_r_squared: 0.5393\n",
      "Epoch 81/500\n",
      "43/43 [==============================] - 0s 720us/step - loss: 0.5753 - mean_absolute_error: 0.4962 - r_squared: 0.5279 - val_loss: 0.7068 - val_mean_absolute_error: 0.5250 - val_r_squared: 0.5235\n",
      "Epoch 82/500\n",
      "43/43 [==============================] - 0s 697us/step - loss: 0.5727 - mean_absolute_error: 0.4957 - r_squared: 0.5124 - val_loss: 0.7023 - val_mean_absolute_error: 0.5252 - val_r_squared: 0.5400\n",
      "Epoch 83/500\n",
      "43/43 [==============================] - 0s 676us/step - loss: 0.5699 - mean_absolute_error: 0.4955 - r_squared: 0.5099 - val_loss: 0.6964 - val_mean_absolute_error: 0.5235 - val_r_squared: 0.5506\n",
      "Epoch 84/500\n",
      "43/43 [==============================] - 0s 673us/step - loss: 0.5670 - mean_absolute_error: 0.4948 - r_squared: 0.5266 - val_loss: 0.6912 - val_mean_absolute_error: 0.5231 - val_r_squared: 0.5314\n",
      "Epoch 85/500\n",
      "43/43 [==============================] - 0s 659us/step - loss: 0.5647 - mean_absolute_error: 0.4951 - r_squared: 0.5394 - val_loss: 0.6865 - val_mean_absolute_error: 0.5227 - val_r_squared: 0.5302\n",
      "Epoch 86/500\n",
      "43/43 [==============================] - 0s 663us/step - loss: 0.5620 - mean_absolute_error: 0.4939 - r_squared: 0.5336 - val_loss: 0.6811 - val_mean_absolute_error: 0.5218 - val_r_squared: 0.5161\n",
      "Epoch 87/500\n",
      "43/43 [==============================] - 0s 675us/step - loss: 0.5597 - mean_absolute_error: 0.4935 - r_squared: 0.5340 - val_loss: 0.6761 - val_mean_absolute_error: 0.5216 - val_r_squared: 0.5591\n",
      "Epoch 88/500\n",
      "43/43 [==============================] - 0s 652us/step - loss: 0.5574 - mean_absolute_error: 0.4930 - r_squared: 0.5356 - val_loss: 0.6720 - val_mean_absolute_error: 0.5209 - val_r_squared: 0.5387\n",
      "Epoch 89/500\n",
      "43/43 [==============================] - 0s 686us/step - loss: 0.5554 - mean_absolute_error: 0.4926 - r_squared: 0.5210 - val_loss: 0.6675 - val_mean_absolute_error: 0.5206 - val_r_squared: 0.5436\n",
      "Epoch 90/500\n",
      "43/43 [==============================] - 0s 662us/step - loss: 0.5535 - mean_absolute_error: 0.4925 - r_squared: 0.5317 - val_loss: 0.6627 - val_mean_absolute_error: 0.5201 - val_r_squared: 0.5490\n",
      "Epoch 91/500\n",
      "43/43 [==============================] - 0s 691us/step - loss: 0.5516 - mean_absolute_error: 0.4920 - r_squared: 0.5084 - val_loss: 0.6585 - val_mean_absolute_error: 0.5193 - val_r_squared: 0.5604\n",
      "Epoch 92/500\n",
      "43/43 [==============================] - 0s 669us/step - loss: 0.5497 - mean_absolute_error: 0.4920 - r_squared: 0.5337 - val_loss: 0.6547 - val_mean_absolute_error: 0.5180 - val_r_squared: 0.5670\n",
      "Epoch 93/500\n",
      "43/43 [==============================] - 0s 707us/step - loss: 0.5481 - mean_absolute_error: 0.4912 - r_squared: 0.5210 - val_loss: 0.6500 - val_mean_absolute_error: 0.5172 - val_r_squared: 0.5612\n",
      "Epoch 94/500\n",
      "43/43 [==============================] - 0s 682us/step - loss: 0.5460 - mean_absolute_error: 0.4902 - r_squared: 0.5350 - val_loss: 0.6453 - val_mean_absolute_error: 0.5160 - val_r_squared: 0.5228\n",
      "Epoch 95/500\n",
      "43/43 [==============================] - 0s 699us/step - loss: 0.5442 - mean_absolute_error: 0.4897 - r_squared: 0.5348 - val_loss: 0.6421 - val_mean_absolute_error: 0.5145 - val_r_squared: 0.5608\n",
      "Epoch 96/500\n",
      "43/43 [==============================] - 0s 693us/step - loss: 0.5426 - mean_absolute_error: 0.4891 - r_squared: 0.5186 - val_loss: 0.6385 - val_mean_absolute_error: 0.5141 - val_r_squared: 0.5305\n",
      "Epoch 97/500\n",
      "43/43 [==============================] - 0s 677us/step - loss: 0.5409 - mean_absolute_error: 0.4883 - r_squared: 0.5313 - val_loss: 0.6340 - val_mean_absolute_error: 0.5126 - val_r_squared: 0.5249\n",
      "Epoch 98/500\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.5392 - mean_absolute_error: 0.4877 - r_squared: 0.5508 - val_loss: 0.6300 - val_mean_absolute_error: 0.5108 - val_r_squared: 0.4912\n",
      "Epoch 99/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5378 - mean_absolute_error: 0.4872 - r_squared: 0.5264 - val_loss: 0.6261 - val_mean_absolute_error: 0.5093 - val_r_squared: 0.5702\n",
      "Epoch 100/500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.5365 - mean_absolute_error: 0.4867 - r_squared: 0.5491 - val_loss: 0.6237 - val_mean_absolute_error: 0.5087 - val_r_squared: 0.5444\n",
      "Epoch 101/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5348 - mean_absolute_error: 0.4855 - r_squared: 0.5317 - val_loss: 0.6205 - val_mean_absolute_error: 0.5084 - val_r_squared: 0.5915\n",
      "Epoch 102/500\n",
      "43/43 [==============================] - 0s 872us/step - loss: 0.5333 - mean_absolute_error: 0.4851 - r_squared: 0.5468 - val_loss: 0.6189 - val_mean_absolute_error: 0.5076 - val_r_squared: 0.6086\n",
      "Epoch 103/500\n",
      "43/43 [==============================] - 0s 687us/step - loss: 0.5324 - mean_absolute_error: 0.4846 - r_squared: 0.5261 - val_loss: 0.6176 - val_mean_absolute_error: 0.5059 - val_r_squared: 0.5867\n",
      "Epoch 104/500\n",
      "43/43 [==============================] - 0s 681us/step - loss: 0.5310 - mean_absolute_error: 0.4838 - r_squared: 0.5413 - val_loss: 0.6176 - val_mean_absolute_error: 0.5059 - val_r_squared: 0.5577\n",
      "Epoch 105/500\n",
      "43/43 [==============================] - 0s 637us/step - loss: 0.5300 - mean_absolute_error: 0.4832 - r_squared: 0.5298 - val_loss: 0.6166 - val_mean_absolute_error: 0.5057 - val_r_squared: 0.5623\n",
      "Epoch 106/500\n",
      "43/43 [==============================] - 0s 698us/step - loss: 0.5289 - mean_absolute_error: 0.4823 - r_squared: 0.5268 - val_loss: 0.6155 - val_mean_absolute_error: 0.5043 - val_r_squared: 0.5847\n",
      "Epoch 107/500\n",
      "43/43 [==============================] - 0s 647us/step - loss: 0.5279 - mean_absolute_error: 0.4817 - r_squared: 0.5401 - val_loss: 0.6138 - val_mean_absolute_error: 0.5039 - val_r_squared: 0.5760\n",
      "Epoch 108/500\n",
      "43/43 [==============================] - 0s 638us/step - loss: 0.5267 - mean_absolute_error: 0.4811 - r_squared: 0.5427 - val_loss: 0.6131 - val_mean_absolute_error: 0.5031 - val_r_squared: 0.5426\n",
      "Epoch 109/500\n",
      "43/43 [==============================] - 0s 677us/step - loss: 0.5258 - mean_absolute_error: 0.4804 - r_squared: 0.5417 - val_loss: 0.6125 - val_mean_absolute_error: 0.5031 - val_r_squared: 0.5427\n",
      "Epoch 110/500\n",
      "43/43 [==============================] - 0s 698us/step - loss: 0.5247 - mean_absolute_error: 0.4800 - r_squared: 0.5533 - val_loss: 0.6117 - val_mean_absolute_error: 0.5023 - val_r_squared: 0.5671\n",
      "Epoch 111/500\n",
      "43/43 [==============================] - 0s 800us/step - loss: 0.5240 - mean_absolute_error: 0.4793 - r_squared: 0.5319 - val_loss: 0.6102 - val_mean_absolute_error: 0.5017 - val_r_squared: 0.6286\n",
      "Epoch 112/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5230 - mean_absolute_error: 0.4790 - r_squared: 0.5606 - val_loss: 0.6092 - val_mean_absolute_error: 0.5012 - val_r_squared: 0.5940\n",
      "Epoch 113/500\n",
      "43/43 [==============================] - 0s 663us/step - loss: 0.5223 - mean_absolute_error: 0.4785 - r_squared: 0.5635 - val_loss: 0.6086 - val_mean_absolute_error: 0.5016 - val_r_squared: 0.5885\n",
      "Epoch 114/500\n",
      "43/43 [==============================] - 0s 667us/step - loss: 0.5215 - mean_absolute_error: 0.4781 - r_squared: 0.5442 - val_loss: 0.6071 - val_mean_absolute_error: 0.4999 - val_r_squared: 0.5179\n",
      "Epoch 115/500\n",
      "43/43 [==============================] - 0s 647us/step - loss: 0.5204 - mean_absolute_error: 0.4772 - r_squared: 0.5503 - val_loss: 0.6059 - val_mean_absolute_error: 0.4996 - val_r_squared: 0.5511\n",
      "Epoch 116/500\n",
      "43/43 [==============================] - 0s 680us/step - loss: 0.5196 - mean_absolute_error: 0.4769 - r_squared: 0.5598 - val_loss: 0.6056 - val_mean_absolute_error: 0.4996 - val_r_squared: 0.5827\n",
      "Epoch 117/500\n",
      "43/43 [==============================] - 0s 668us/step - loss: 0.5187 - mean_absolute_error: 0.4763 - r_squared: 0.5559 - val_loss: 0.6041 - val_mean_absolute_error: 0.4984 - val_r_squared: 0.5700\n",
      "Epoch 118/500\n",
      "43/43 [==============================] - 0s 685us/step - loss: 0.5181 - mean_absolute_error: 0.4760 - r_squared: 0.5517 - val_loss: 0.6032 - val_mean_absolute_error: 0.4978 - val_r_squared: 0.5477\n",
      "Epoch 119/500\n",
      "43/43 [==============================] - 0s 690us/step - loss: 0.5172 - mean_absolute_error: 0.4753 - r_squared: 0.5553 - val_loss: 0.6020 - val_mean_absolute_error: 0.4973 - val_r_squared: 0.6265\n",
      "Epoch 120/500\n",
      "43/43 [==============================] - 0s 695us/step - loss: 0.5169 - mean_absolute_error: 0.4750 - r_squared: 0.5303 - val_loss: 0.6005 - val_mean_absolute_error: 0.4960 - val_r_squared: 0.4997\n",
      "Epoch 121/500\n",
      "43/43 [==============================] - 0s 659us/step - loss: 0.5157 - mean_absolute_error: 0.4744 - r_squared: 0.5393 - val_loss: 0.6005 - val_mean_absolute_error: 0.4970 - val_r_squared: 0.5354\n",
      "Epoch 122/500\n",
      "43/43 [==============================] - 0s 645us/step - loss: 0.5149 - mean_absolute_error: 0.4739 - r_squared: 0.5543 - val_loss: 0.5992 - val_mean_absolute_error: 0.4969 - val_r_squared: 0.5477\n",
      "Epoch 123/500\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.8635 - mean_absolute_error: 0.6202 - r_squared: 0.2706"
     ]
    }
   ],
   "source": [
    "# Callbacks & Tensorboard Setup\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Keras Tuner Design\n",
    "def model_builder(hp):\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    l1_regularization = hp.Float('l1_regularization', min_value=1e-5, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "    l2_regularization =  hp.Float('l2_regularization', min_value=1e-5, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(units = 1, \n",
    "            input_dim=X_train_d.shape[1], \n",
    "            kernel_regularizer=L1L2(l1_regularization, l2_regularization))\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                loss='mean_squared_error', \n",
    "                metrics=['mean_absolute_error', r_squared])\n",
    "    return model\n",
    "\n",
    "# Create a Keras Tuner\n",
    "dense_lr_tuner = kt.RandomSearch(\n",
    "    hypermodel = model_builder,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='logs/flights_ontime/dense_lr/',\n",
    "    project_name='tuner',\n",
    "    overwrite = True\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters\n",
    "dense_lr_tuner.search(train_ds_flights_ontime_d, \n",
    "             validation_data=val_ds_flights_ontime_d, \n",
    "             epochs=500, \n",
    "             callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Neuron LR model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in logs/flights_ontime/dense_lr/tuner\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.008838843007600083\n",
      "l1_regularization: 3.7685531425442215e-05\n",
      "l2_regularization: 1.372387064892845e-05\n",
      "Score: 0.41170233488082886\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.006377919119726471\n",
      "l1_regularization: 0.0005978766454671086\n",
      "l2_regularization: 1.5554077728561435e-05\n",
      "Score: 0.42575952410697937\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.005682201189419717\n",
      "l1_regularization: 0.0003781487250481495\n",
      "l2_regularization: 0.009060936985623536\n",
      "Score: 0.43453511595726013\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0006588156968074904\n",
      "l1_regularization: 1.3806428858696358e-05\n",
      "l2_regularization: 7.501389510842734e-05\n",
      "Score: 0.4362606108188629\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00019955413624931227\n",
      "l1_regularization: 4.368943790310963e-05\n",
      "l2_regularization: 0.00023083171717082758\n",
      "Score: 0.4416203796863556\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.009821864529330936\n",
      "l1_regularization: 0.0012308559709762405\n",
      "l2_regularization: 0.0010866874437649742\n",
      "Score: 0.45288047194480896\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0043647340332005746\n",
      "l1_regularization: 0.00615143664739684\n",
      "l2_regularization: 0.0003444544437591578\n",
      "Score: 0.45623978972435\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0006738022111486705\n",
      "l1_regularization: 1.4530635783292395e-05\n",
      "l2_regularization: 0.0734205586550341\n",
      "Score: 0.4605698883533478\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.000675099141750619\n",
      "l1_regularization: 1.795003368879708e-05\n",
      "l2_regularization: 0.07288228073974409\n",
      "Score: 0.4656814634799957\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0052437766851000074\n",
      "l1_regularization: 0.0011598367621254103\n",
      "l2_regularization: 0.07981573963797062\n",
      "Score: 0.4697344899177551\n"
     ]
    }
   ],
   "source": [
    "# Print hyperparameters for the 10 best trials\n",
    "dense_lr_tuner.results_summary(num_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Hyperparameters:\n",
      "- Learning Rate: 0.009\n",
      "- L1 Regularization: 0.00004\n",
      "- L2 Regularization: 0.00001\n",
      "\n",
      "Loss and Metrics for Best Trial:\n",
      "- Validation Loss: 0.41\n",
      "- Validation MAE: 0.44\n",
      "- Validation R^2: 0.522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get best hyperparameters\n",
    "best_hps = dense_lr_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "learning_rate = best_hps.get('learning_rate')\n",
    "l1_amount = best_hps.get('l1_regularization')\n",
    "l2_amount = best_hps.get('l2_regularization')\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Hyperparameters:\n",
    "- Learning Rate: {learning_rate:.3f}\n",
    "- L1 Regularization: {l1_amount:.5f}\n",
    "- L2 Regularization: {l2_amount:.5f}\n",
    "\"\"\")\n",
    "\n",
    "# Get best trial\n",
    "best_trial = dense_lr_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "\n",
    "# Best trial metrics\n",
    "val_loss = best_trial.metrics.get_best_value('val_loss')\n",
    "val_mae = best_trial.metrics.get_best_value('val_mean_absolute_error')\n",
    "val_r2 = best_trial.metrics.get_best_value('val_r_squared')\n",
    "\n",
    "print(f\"\"\"Loss and Metrics for Best Trial:\n",
    "- Validation Loss: {val_loss:.2f}\n",
    "- Validation MAE: {val_mae:.2f}\n",
    "- Validation R^2: {val_r2:.3f}\n",
    "\"\"\")\n",
    "\n",
    "# Tensorboard directory setup\n",
    "!rm -rf ./logs/flights_ontime/dense_lr/tensorboard/ \n",
    "log_dir = \"logs/flights_ontime/dense_lr/tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train the model with the optimal hyperparameters\n",
    "model = dense_lr_tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_ds_flights_ontime_d, \n",
    "                    validation_data=val_ds_flights_ontime_d, \n",
    "                    epochs=500, \n",
    "                    callbacks=[early_stopping, tensorboard_callback],\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1-Neuron LR TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 92391), started 0:00:02 ago. (Use '!kill 92391' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-46a7f0dc8771167a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-46a7f0dc8771167a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/flights_ontime/dense_lr/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORECASTING WITH A SHALLOW DENSE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 03s]\n",
      "val_loss: 0.4693162441253662\n",
      "\n",
      "Best val_loss So Far: 0.4221741557121277\n",
      "Total elapsed time: 00h 00m 13s\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int('n_hidden', min_value=1, max_value=2, default=2)\n",
    "    n_neurons = hp.Int('n_neurons', min_value=1, max_value=32, default=16)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.02, max_value=0.03, default=0.0)\n",
    "    l2_regularization =  hp.Float('l2_regularization', min_value=1e-5, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=n_neurons, \n",
    "                    activation='relu', \n",
    "                    kernel_regularizer=L2(l2_regularization)))\n",
    "    \n",
    "    for layer in range(n_hidden-1):\n",
    "        model.add(Dense(units=n_neurons, \n",
    "                        activation='relu', \n",
    "                        kernel_regularizer=L2(l2_regularization)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    if n_hidden > 0:\n",
    "        model.add(Dense(units=n_neurons, \n",
    "                        activation='relu', \n",
    "                        kernel_regularizer=L2(l2_regularization)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                  loss='mean_squared_error', \n",
    "                  metrics=['mean_absolute_error'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel = build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    num_initial_points=2,\n",
    "    directory = \"flights_ontime\",\n",
    "    project_name = \"flights_ontime_shallow_dense_fit\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "dense_shallow_tuner = kt.RandomSearch(\n",
    "    hypermodel = build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    directory = \"logs/flights_ontime/dense_shallow/\",\n",
    "    project_name = \"tuner\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "dense_shallow_tuner.search(train_ds_flights_ontime_d, \n",
    "             epochs=500, \n",
    "             validation_data=val_ds_flights_ontime_d, \n",
    "             callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Dense NN model perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in flights_ontime/flights_ontime_shallow_dense_fit\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 3 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 1\n",
      "n_neurons: 32\n",
      "learning_rate: 0.00041697231056181166\n",
      "dropout_rate: 0.02562061484564263\n",
      "l2_regularization: 1.4209495255444217e-05\n",
      "optimizer: rmsprop\n",
      "Score: 0.4221741557121277\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 6\n",
      "learning_rate: 0.0021840106813786914\n",
      "dropout_rate: 0.023538920452711963\n",
      "l2_regularization: 0.0001279036661122155\n",
      "optimizer: rmsprop\n",
      "Score: 0.46542906761169434\n",
      "\n",
      "Trial 4 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 4\n",
      "learning_rate: 0.0008024046237377111\n",
      "dropout_rate: 0.025398744946381654\n",
      "l2_regularization: 0.007335252091684161\n",
      "optimizer: adam\n",
      "Score: 0.4693162441253662\n",
      "\n",
      "Trial 1 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 1\n",
      "n_neurons: 21\n",
      "learning_rate: 0.006439485147843038\n",
      "dropout_rate: 0.02250353639255253\n",
      "l2_regularization: 0.08388836483286709\n",
      "optimizer: adam\n",
      "Score: 0.4800547957420349\n",
      "\n",
      "Trial 2 summary\n",
      "Hyperparameters:\n",
      "n_hidden: 2\n",
      "n_neurons: 12\n",
      "learning_rate: 0.0047344570764701\n",
      "dropout_rate: 0.022261265180403104\n",
      "l2_regularization: 0.09547031092570946\n",
      "optimizer: adam\n",
      "Score: 0.5291304588317871\n"
     ]
    }
   ],
   "source": [
    "# Print hyperparameters for the 10 best trials\n",
    "dense_shallow_tuner.results_summary(num_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = dense_shallow_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "# Get best hyperparameters\n",
    "n_hidden = best_hps.get('n_hidden')\n",
    "n_neurons = best_hps.get('n_neurons')\n",
    "learning_rate = best_hps.get('learning_rate')\n",
    "dropout_rate = best_hps.get('dropout_rate')\n",
    "l2_amount = best_hps.get('l2_regularization')\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Hyperparameters:\n",
    "- Number of Hidden Layers: {n_hidden}\n",
    "- Number of Neurons: {n_neurons}\n",
    "- Learning Rate: {learning_rate:.3f}\n",
    "- Dropout Rate: {dropout_rate:.3f}\n",
    "- L2 Regularization: {l2_amount:.5f}\n",
    "\"\"\")\n",
    "\n",
    "# Get best trial\n",
    "best_trial = dense_shallow_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "\n",
    "# Best trial metrics\n",
    "val_loss = best_trial.metrics.get_best_value('val_loss')\n",
    "val_mae = best_trial.metrics.get_best_value('val_mean_absolute_error')\n",
    "\n",
    "print(f\"\"\"Loss and Metrics for Best Trial:\n",
    "- Validation Loss: {val_loss:.2f}\n",
    "- Validation MAE: {val_mae:.2f}\n",
    "\"\"\")\n",
    "\n",
    "# Tensorboard directory setup\n",
    "!rm -rf ./logs/flights_ontime/dense_shallow/tensorboard/ \n",
    "log_dir = \"logs/flights_ontime/dense_shallow/tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Change early stopping patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Train the model with the optimal hyperparameters\n",
    "model = dense_shallow_tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_ds_flights_ontime_d, \n",
    "                    validation_data=val_ds_flights_ontime_d, \n",
    "                    epochs=500, \n",
    "                    callbacks=[early_stopping, tensorboard_callback],\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMESERIES WITH RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove lag variables from X train, val, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_X_train_full = X_train_full.drop(lag_cols, axis=1)\n",
    "rnn_X_train = X_train.drop(lag_cols, axis=1)\n",
    "rnn_X_val = X_val.drop(lag_cols, axis=1)\n",
    "rnn_X_test = X_test.drop(lag_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN column transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_numeric_cols = [col for col in numeric_cols if col not in lag_cols]\n",
    "\n",
    "# Fit transformers to the training data\n",
    "rnn_f_scaler = StandardScaler()\n",
    "rnn_f_scaler.fit(rnn_X_train[rnn_numeric_cols])\n",
    "\n",
    "# Create a function to preprocess TensorFlow datasets\n",
    "def rnn_preprocess(features, target):\n",
    "    scaled_features = rnn_f_scaler.transform(features[rnn_numeric_cols])\n",
    "    encoded_features = ohe.transform(features[categorical_cols])\n",
    "    scaled_target = t_scaler.transform(target.values.reshape(-1, 1))\n",
    "    processed_features = np.concatenate([scaled_features, encoded_features], axis=1)\n",
    "    return processed_features, scaled_target\n",
    "\n",
    "# Transform the data\n",
    "X_train_rnn, y_train_rnn = rnn_preprocess(X_train, y_train)\n",
    "X_val_rnn, y_val_rnn = rnn_preprocess(X_val, y_val)\n",
    "X_test_rnn, y_test_rnn = rnn_preprocess(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create timeseries datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "seq_length = 7\n",
    "batch_size = 32\n",
    "\n",
    "train_rnn = timeseries_dataset_from_array(\n",
    "    data = X_train_rnn, \n",
    "    targets = y_train_rnn,\n",
    "    sequence_length = seq_length,\n",
    "    sequence_stride = 1,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "val_rnn = timeseries_dataset_from_array(\n",
    "    data = X_val_rnn, \n",
    "    targets = y_val_rnn[seq_length-1:],\n",
    "    sequence_length = seq_length,\n",
    "    sequence_stride = 1,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "test_rnn = timeseries_dataset_from_array(\n",
    "    data = X_test_rnn, \n",
    "    targets = y_test_rnn[seq_length-1:],\n",
    "    sequence_length = seq_length,\n",
    "    sequence_stride = 1,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Using a single neuron RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# The validation MAE still varies from one run to another. GPU may impart some randomness to the results.\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    SimpleRNN(\n",
    "        units=32,\n",
    "        input_shape=(None, X_train_rnn.shape[1]),\n",
    "        kernel_regularizer=regularizers.l2(0.01),\n",
    "        recurrent_regularizer=regularizers.l2(0.01),\n",
    "        activation='relu'\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        ])\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "# opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/rnn_1_flights_ontime/\n",
    "\n",
    "log_dir = \"logs/rnn_1_flights_ontime/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_rnn,\n",
    "    epochs=5000,\n",
    "    validation_data=val_rnn,\n",
    "    callbacks=[tensorboard_callback, early_stopping]\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill 220\n",
    "%tensorboard --logdir logs/rnn_1_flights_ontime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting using a shallow RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int('n_hidden', min_value=1, max_value=2, default=2)\n",
    "    n_neurons = hp.Int('n_neurons', min_value=1, max_value=32, default=16)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.02, max_value=0.03, default=0.0)\n",
    "    regularization_type = hp.Choice('regularization_type', values=['l1', 'l2', 'l1_l2'], default='l2')\n",
    "    l1_regularization = hp.Float('l1_regularization', min_value=1e-4, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "    l2_regularization = hp.Float('l2_regularization', min_value=1e-4, max_value=1e-1, sampling='LOG', default=1e-2)\n",
    "\n",
    "\n",
    "    if regularization_type == 'l1':\n",
    "        regularizer = l1(l1_regularization)\n",
    "    elif regularization_type == 'l2':\n",
    "        regularizer = l2(l2_regularization)\n",
    "    else:\n",
    "        regularizer = l1_l2(l1=l1_regularization, l2=l2_regularization)\n",
    "\n",
    "   \n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'], default='adam')\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=n_neurons, input_shape=(None, X_train_rnn.shape[1]), activation='relu', return_sequences=True, kernel_regularizer=regularizer))\n",
    "    for layer in range(n_hidden-1):\n",
    "        model.add(SimpleRNN(units=n_neurons, activation='relu', return_sequences=True, kernel_regularizer=regularizer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    if n_hidden > 0:\n",
    "        model.add(SimpleRNN(units=n_neurons, activation='relu', kernel_regularizer=regularizer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#     build_model,\n",
    "#     objective='val_loss',\n",
    "#     max_trials=50,\n",
    "#     num_initial_points=2,\n",
    "#     overwrite=True\n",
    "# )\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(train_rnn, epochs=500, validation_data=val_rnn, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_3hps = tuner.get_best_hyperparameters(num_trials=3)\n",
    "\n",
    "print(\"Best 3 hyperparameter sets:\")\n",
    "print(best_3hps[0].values)\n",
    "print(best_3hps[1].values)\n",
    "print(best_3hps[2].values, '\\n')\n",
    "\n",
    "\n",
    "best_trial = tuner.oracle.get_best_trials(1)[0]\n",
    "\n",
    "best_trial.summary()\n",
    "\n",
    "print(\"\\nBest trial validation loss\", best_trial.metrics.get_last_value('val_loss'))\n",
    "print(\"Best trial validation MAE\", best_trial.metrics.get_last_value('mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. add TensorBoard to RNN\n",
    "2. Add L1 and L2 regularization to Keras Tuner \n",
    "3. Tune for 500 trials (overnight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prediction-rTmYhf-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
